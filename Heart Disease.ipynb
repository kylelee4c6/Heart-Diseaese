{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Presence of Heart Disease\n",
    "**This notebook will explore a heart disease database collected from the University of California, Irvine Machine Learning Repository. I want to compare various machine learning algorithms as well as tune the most promising models in order to maximize the accuracy of predicting the presence of heart disease. This has various applications in the medical field of detecting patients who are mostly likely to experience a heart disease in the near future given a set of features such as age, sex, and chest pains.** \n",
    "\n",
    "\n",
    "The dataset can be found here: [UCI Machine Learning Repository - Statlog (Heart)](http://archive.ics.uci.edu/ml/heart_diseases/statlog+(heart))\n",
    "\n",
    "I will be running the following models:\n",
    "1. Logistic Regression\n",
    "2. Decision trees\n",
    "3. K-Nearest Neighbors\n",
    "4. Linear Discriminant Analysis\n",
    "5. Gaussian Naive Bayes\n",
    "6. Support Vector Machines\n",
    "\n",
    "as well as have a 70/30 train/test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load heart_diseases\n",
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/heart/heart.dat\"\n",
    "heart_disease = pd.read_fwf(url, header=None)\n",
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column 3 has most of our features so we split the column. Then, I create dummy binary variables for chest pain levels, ecg levels, and thalamus levels since they are categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data =heart_disease[3].str.split('\\s+', expand = True)\n",
    "del heart_disease[3]\n",
    "heart_disease = pd.concat([heart_disease, split_data], axis = 1, ignore_index = True)\n",
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store and convert categorical variables into floats\n",
    "chest_pain  =heart_disease[2].astype('float64')\n",
    "ecg = heart_disease[6].astype('float64')\n",
    "thal = heart_disease[12].astype('float64')\n",
    "\n",
    "# Binarize categorical variables\n",
    "chest_pain_bin = pd.get_dummies(chest_pain)\n",
    "ecg_bin = pd.get_dummies(ecg)\n",
    "thal_bin = pd.get_dummies(thal)\n",
    "response = heart_disease[13]\n",
    "\n",
    "# Delete columns\n",
    "del heart_disease[13]\n",
    "del heart_disease[12]\n",
    "del heart_disease[6]\n",
    "del heart_disease[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mix partitioned data\n",
    "heart_disease = pd.concat([heart_disease, chest_pain_bin, ecg_bin, thal_bin, response], axis = 1, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure quality control\n",
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at datatypes\n",
    "pd.set_option('display.max_rows',500)\n",
    "heart_disease.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data types into float64 for arithmetic operations\n",
    "heart_disease.iloc[:, 0:20] = heart_disease.iloc[:, 0:20].astype('float64')\n",
    "heart_disease.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set precision to 3 decimal places\n",
    "pd.set_option('precision', 3)\n",
    "heart_disease.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we have encoded binary dummy variables. So far we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Index | Column   |\n",
    "|------|-----------------|\n",
    "|   0  | Age|\n",
    "| 1 | Sex (1-Male, 0-Female)|\n",
    "|2 | Resting Blood Pressure |\n",
    "|3 | Serum Cholesterol in mg/dl |\n",
    "| 4 | Fasting blood sugar | \n",
    "| 5 | Maximum heart rate achieved |\n",
    "| 6 | Exercise induced angina | \n",
    "| 7 | Old peak ST depression induced by exercise relative to rest |\n",
    "| 8 | Slope of the peak exercise ST segment |\n",
    "| 9 | Number of major vessels (0-3) colored by flourosopy |\n",
    "|10 | (BINARY) Chest Pain - 1 |\n",
    "| 11 | (BINARY) Chest Pain - 2 |\n",
    "| 12 | (BINARY) Chest Pain - 3 |\n",
    "| 13| (BINARY) Chest Pain - 4 | \n",
    "| 14 | (BINARY) Resting Electrocardiographic results - 0 |\n",
    "| 15 | (BINARY) Resting Electrocardiographic results - 1 |\n",
    "| 16 | (BINARY) Resting Electrocardiographic results - 2 |\n",
    "| 17 | (BINARY) Thalamus: Normal (3) |\n",
    "| 18 | (BINARY) Thalamus: Fixed defect (6) |\n",
    "| 19 | (BINARY) Thalamus: Reversable defect ( 7) |\n",
    "| 20 | Absence (1) or presence (2) of heart disease |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Create histograms\n",
    "heart_disease.hist( column = [0,1,2,3,4,5,6,7,8,9],sharex = False, sharey = False, xlabelsize = 1, ylabelsize = 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that age reasonably follows a normal distribution. Also, resting blood pressure at the time the data was taken seems to follow a right skewed normal distribution. The levels of exercised induced angina also follows a right skewed normal distribution. Slope of the peak exercise ST segment is reasonably a normal distribution. Note that we have a class imbalance in sex, fasting blood sugar, number of major vessels colored by flouroscopy, and slope of the peaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram based on chest pain levels\n",
    "plt.bar(range(1,5),chest_pain.value_counts().sort_index())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barplot based on electrocardiogram levels (0,1,2)\n",
    "plt.bar(range(1,4),ecg.value_counts().sort_index())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barplot based on thalamus\n",
    "plt.bar(range(1,4), thal.value_counts().sort_index())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Density Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create density plots on non-binary data\n",
    "heart_disease.iloc[:, [0,2,3,5,7,9]].plot(kind='density', subplots=True, layout=(3,4), sharex=False, legend=False, fontsize=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation matrix\n",
    "In order to prevent the confusion between the binary variables I have encoded, I have decided to leave them off in order to see some basic relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See correlation between non-binary data\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(heart_disease.iloc[:, [0,2,3,5,7,9]].corr(), vmin=-1, vmax=1, interpolation='none')\n",
    "fig.colorbar(cax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation dataset\n",
    "I decide to have train/test split of 70/30 in order to get some reasonable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "array = heart_disease.values\n",
    "X = array[:,0:19].astype(float)\n",
    "Y = array[:,20]\n",
    "validation_size = 0.3\n",
    "seed = 7\n",
    "X_train, X_validation, Y_train, Y_validation = cross_validation.train_test_split(X, Y,test_size=validation_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Algorithms\n",
    "I also perform a 15-fold cross-validation and use the accuracy of classification as my performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test options and evaluation metric\n",
    "num_folds = 15\n",
    "num_instances = len(X_train)\n",
    "seed = 7\n",
    "scoring_accuracy = 'accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store models\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LR, LDA, KNN, CART, NB, SVM on data\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = cross_validation.KFold(n = num_instances, n_folds = num_folds, random_state = seed)\n",
    "    cv_results = cross_validation.cross_val_score(model, X_train, Y_train, cv = kfold, scoring = scoring_accuracy)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Discriminant Analysis (LDA) gives us a warning that certain predictors are collinear. This makes sense due to our binary encoding of the variables. For example, an increase in scoring a 1 on chest pain implies a decrease on chest pains of 2,3,4 and so it underestimates the effect of scoring a 1 on heart disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Algorithms\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize data due to distributions\n",
    "Note that variables such as age carry the most weight in the classification process and so I decide to standardize the data points to the standard normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the heart_disease\n",
    "pipelines = []\n",
    "pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR', LogisticRegression())])))\n",
    "pipelines.append(('ScaledLDA', Pipeline([('Scaler', StandardScaler()),('LDA', LinearDiscriminantAnalysis())])))\n",
    "pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsClassifier())])))\n",
    "pipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeClassifier())])))\n",
    "pipelines.append(('ScaledNB', Pipeline([('Scaler', StandardScaler()),('NB', GaussianNB())])))\n",
    "pipelines.append(('ScaledSVM', Pipeline([('Scaler', StandardScaler()),('SVM', SVC())])))\n",
    "results = []\n",
    "names = []\n",
    "for name, model in pipelines:\n",
    "    kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n",
    "    cv_results = cross_validation.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring_accuracy)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Algorithms\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Scaled Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune k Nearest neighbors\n",
    "By default, k-Nearest Neighbors runs 5 neighbors so I'd like to tune the parameter to see if we can achieve better accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "neighbors = [1,3,5,7,9,11,13,15,17,19,21]\n",
    "\n",
    "param_grid = dict(n_neighbors=neighbors)\n",
    "model = KNeighborsClassifier()\n",
    "kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring_accuracy, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "for params, mean_score, scores in grid_result.grid_scores_:\n",
    "    print(\"%f (%f) with: %r\" % (scores.mean(), scores.std(), params))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning Logistic Regression\n",
    "Logistic Regression showed us the best promise and so by changing the hyperparameter, C, I hope to achieve better accuracy as we cross-validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tune Logistic Regression\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "param_grid = {'C': [.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000] }\n",
    "kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n",
    "grid = GridSearchCV(estimator=LogisticRegression(), param_grid=param_grid, scoring=scoring_accuracy, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "for params, mean_score, scores in grid_result.grid_scores_:\n",
    "    print(\"%f (%f) with: %r\" % (scores.mean(), scores.std(), params))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning Support Vector Machine\n",
    "SVM is one of those algorithms that can perform well if the kernel and hyperparameter, C, is tuned. Therefore, we perform a grid search to try various combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tune SVM parameter c as well as kernels\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "c_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0]\n",
    "kernel_values = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "param_grid = dict(C=c_values, kernel=kernel_values)\n",
    "\n",
    "kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n",
    "grid = GridSearchCV(estimator=SVC(), param_grid=param_grid, scoring=scoring_accuracy, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "for params, mean_score, scores in grid_result.grid_scores_:\n",
    "    print(\"%f (%f) with: %r\" % (scores.mean(), scores.std(), params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run emsemble methods\n",
    "ensembles = []\n",
    "ensembles.append(('AB', AdaBoostClassifier()))\n",
    "ensembles.append(('GBM', GradientBoostingClassifier()))\n",
    "ensembles.append(('RF', RandomForestClassifier()))\n",
    "ensembles.append(('ET', ExtraTreesClassifier()))\n",
    "results = []\n",
    "names = []\n",
    "for name, model in ensembles:\n",
    "    kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n",
    "    cv_results = cross_validation.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring_accuracy)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.suptitle('Ensemble Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance on the Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test LR model with C = .01\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "model = LogisticRegression(C=.01)\n",
    "model.fit(rescaledX, Y_train)\n",
    "\n",
    "# Test on cross validation set\n",
    "rescaledValidationX = scaler.transform(X_validation)\n",
    "predictions = model.predict(rescaledValidationX)\n",
    "print(\"Logistic Regression\")\n",
    "print(accuracy_score(Y_validation, predictions))\n",
    "print(confusion_matrix(Y_validation, predictions))\n",
    "print(classification_report(Y_validation, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SVC with hyperparameter C= .03 and rbf \n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "model2 = SVC(C=.5, kernel = \"rbf\")\n",
    "model2.fit(rescaledX, Y_train)\n",
    "predictions2 = model2.predict(rescaledValidationX)\n",
    "\n",
    "print(\"Support Vector Machine\")\n",
    "print(accuracy_score(Y_validation, predictions2))\n",
    "print(confusion_matrix(Y_validation, predictions2))\n",
    "print(classification_report(Y_validation, predictions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test kNN with 17 neighbors\n",
    "model3 =  KNeighborsClassifier(n_neighbors = 17)\n",
    "model3.fit(rescaledX, Y_train)\n",
    "predictions3 = model3.predict(rescaledValidationX)\n",
    "\n",
    "print( \"k-Nearest Neighbors\")\n",
    "print(accuracy_score(Y_validation, predictions3))\n",
    "print(confusion_matrix(Y_validation, predictions3))\n",
    "print(classification_report(Y_validation, predictions3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conclude that by using logistic regression, we can achieve 84% accuracy in our model after fine tuning. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
